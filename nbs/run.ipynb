{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b44551e",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192895f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload your package\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae72038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "# logger.remove()\n",
    "# import sys\n",
    "# logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198de680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T02:34:01.879987Z",
     "start_time": "2022-06-28T02:34:01.864103Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (prompt_loading.py, line 310)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/media/wassname/SGIronWolf/projects5/elk/lie_elicitation_prompts/lie_elicitation_prompts/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 10\u001b[0;36m\n\u001b[0;31m    from lie_elicitation_prompts.prompts.prompt_loading import load_preproc_datasets, load_prompts\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/lie_elicitation_prompts/lie_elicitation_prompts/lie_elicitation_prompts/prompts/prompt_loading.py:310\u001b[0;36m\u001b[0m\n\u001b[0;31m    M=<n\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import lie_elicitation_prompts\n",
    "from lie_elicitation_prompts.config import ExtractConfig\n",
    "from lie_elicitation_prompts.helpers.scores import row_choice_ids\n",
    "from lie_elicitation_prompts.prompts.prompt_loading import load_preproc_datasets, load_prompts\n",
    "\n",
    "cfg = ExtractConfig()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ce98c",
   "metadata": {},
   "source": [
    "## Load text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # debug\n",
    "# for ds_name in cfg.datasets:\n",
    "#     print(ds_name)\n",
    "#     o = load_prompts(ds_name, num_shots=1, N=2) \n",
    "#     o = list(tqdm(o))\n",
    "#     # print(ds_name, o)\n",
    "#     1/0\n",
    "# pd.DataFrame(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa8f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ignore UserWarning category\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"^The groups parameter is ignored by StratifiedShuffleSplit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N = cfg.max_examples\n",
    "ds_prompts = load_preproc_datasets(\n",
    "    cfg.datasets,\n",
    "    N=N,\n",
    "    seed=cfg.seed,\n",
    "    num_shots=cfg.num_shots,\n",
    "    M=cfg.repeats,\n",
    ")\n",
    "ds_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe1b46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90868bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_prompts_ood = load_preproc_datasets(\n",
    "#     cfg.datasets_ood,\n",
    "#     N=N,\n",
    "#     seed=cfg.seed,\n",
    "#     num_shots=cfg.num_shots,\n",
    "# )\n",
    "# ds_prompts_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6334ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prompts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058982f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1050f5",
   "metadata": {},
   "source": [
    "## Load tokenized dataset\n",
    "\n",
    "- tokenize\n",
    "- filter out truncated\n",
    "- check which ones the model knows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07503ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds_tokens = (\n",
    "    ds_prompts.map(\n",
    "        lambda x: {\n",
    "            \"formatted_chat\": tokenizer.apply_chat_template(\n",
    "                x[\"messages\"], tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    .map(\n",
    "        lambda x: tokenizer(\n",
    "            x[\"formatted_chat\"],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=cfg.max_tokens,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        ),\n",
    "        batched=True,\n",
    "    )\n",
    "    .map(lambda r: {\"choice_ids\": row_choice_ids(r, tokenizer)}, desc=\"choice_ids\")\n",
    "    .filter(lambda x: x[\"attention_mask\"].sum() < cfg.max_tokens)\n",
    ")\n",
    "ds_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ece49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def apply_prompt(messages):\n",
    "#     o = tokenizer.apply_chat_template(\n",
    "#                 messages, add_generation_prompt=True, return_tensors=\"pt\",\n",
    "#             max_length=cfg.max_tokens,\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#             return_dict=True,\n",
    "#             )\n",
    "#     return {k:v.squeeze() for k,v in o.items()}\n",
    "\n",
    "# ds_tokens = (\n",
    "#     ds_prompts.map(\n",
    "#         lambda x: apply_prompt(x[\"messages\"])\n",
    "#     )\n",
    "#     .map(lambda r: {\"choice_ids\": row_choice_ids(r, tokenizer)}, desc=\"choice_ids\")\n",
    "#     .filter(lambda x: x[\"attention_mask\"].sum() < cfg.max_tokens)\n",
    "# )\n",
    "# ds_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_prompts), len(ds_tokens))\n",
    "\n",
    "pd.Series(ds_prompts['ds_string']).value_counts(), pd.Series(ds_tokens['ds_string']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "ds_tokens[:4]['formatted_chat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8669c0",
   "metadata": {},
   "source": [
    "### Check model knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = ds_tokens.select_columns(['ds_string', 'example_i', 'sys_instr_name', 'instructed_to_lie']).to_pandas().reset_index(names='my_ds_index')\n",
    "df_metadata_truth = df_metadata.query('instructed_to_lie == False')\n",
    "df_metadata_truth\n",
    "\n",
    "# FIXME right now there is just one example of each, I guess I want a couple, hmm\n",
    "df_metadata.query('instructed_to_lie == False').groupby(['ds_string', 'example_i'], as_index=False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed668740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get a single example of a truthful response to each question\n",
    "# df_metadata = ds_tokens.select_columns(['ds_string', 'example_i', 'sys_instr_name', 'instructed_to_lie']).to_pandas().reset_index(names='my_ds_index')\n",
    "# df_metadata_truth = df_metadata.query('instructed_to_lie == False').groupby(['ds_string', 'example_i'], as_index=False).first()\n",
    "# df_metadata_truth\n",
    "\n",
    "# ds_tokens_truthful = ds_tokens.select(df_metadata_truth.my_ds_index)\n",
    "# ds_tokens_truthful\n",
    "\n",
    "ds_tokens_truthful = ds_tokens.select(torch.argwhere(~ds_tokens['instructed_to_lie']))\n",
    "ds_tokens_truthful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lie_elicitation_prompts.helpers.torch_helpers import clear_mem\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15125f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, in some dataset we get 50, so totally random. We need to rephrase the same question multiple ways to avoid this\n",
    "# also check example i refers to the question, not the generated prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from lie_elicitation_prompts.helpers.scores import sum_select_choices_from_logits\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "ds = ds_tokens_truthful.select_columns(['ds_string', 'example_i', 'label_true', 'input_ids', 'attention_mask', 'choice_ids'])\n",
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "results = []\n",
    "\n",
    "for nb, batch in enumerate(tqdm(dl)):\n",
    "\n",
    "    # to device\n",
    "    inputs = {'input_ids': batch['input_ids'].to(model.device), 'attention_mask': batch['attention_mask'].to(model.device)}\n",
    "    labels = batch['label_true']\n",
    "    choice_ids = batch['choice_ids'].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "\n",
    "        logits_last = out['logits'][:, -1].detach().cpu()\n",
    "        p = out['choice_llm_probs'] = sum_select_choices_from_logits(logits_last, choice_ids)\n",
    "        out['prob_bool'] = p[:, 1] / (torch.sum(p, 1) + 1e-12) # bool prob is the probability of the second choice\n",
    "        corrects = labels==(out['prob_bool']>0.5)\n",
    "\n",
    "        for batch_i, correct in enumerate(corrects):\n",
    "            if batch_i==0 and nb==0:\n",
    "                # print(i, correct, batch['formatted_prompt'][batch_i])\n",
    "                # s = tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)[batch_i]\n",
    "                print(nb, nb, correct.item(), out['choice_llm_probs'][:, 1])\n",
    "            results.append({\n",
    "                'ds_string': batch['ds_string'][batch_i],\n",
    "                'example_i': batch['example_i'][batch_i].item(),\n",
    "                'correct': correct.item(),\n",
    "                'prob_bool': out['prob_bool'][batch_i].item(),\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f7bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28821251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work out which question it knows the answer too\n",
    "df_res = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "acc = df_res.groupby('ds_string').correct.mean()\n",
    "print(f\"Accuracy: {acc.to_dict()}\")\n",
    "\n",
    "# TODO we need to make sure it got all version right, not just one\n",
    "\n",
    "df_known = df_res[df_res.correct][['ds_string', 'example_i']]\n",
    "df_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74223660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f02ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_is_known(x):\n",
    "    k = df_known[df_known.ds_string==x['ds_string']]\n",
    "    return x['example_i'].item() in k.example_i.values\n",
    "\n",
    "# filter the dataset to known answers based on ds_string and example_i\n",
    "ds_tokens_known = ds_tokens.filter(row_is_known)\n",
    "print(f\"{len(ds_tokens)} -> {len(ds_tokens_known)}\")\n",
    "ds_tokens_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa14959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "ts = pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')\n",
    "f = Path(f\"../data/extracted_prompts_{ts}\")\n",
    "print(f)\n",
    "ds_tokens_known.info.description = json.dumps(cfg.__dict__)\n",
    "ds_tokens_known.save_to_disk(str(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63249bf",
   "metadata": {},
   "source": [
    "## QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it correct, or is it random guessing?\n",
    "acc = df_res.groupby('ds_string').correct.mean()\n",
    "print(f\"Accuracy: {acc.to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd63799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which source datasets did the known questions come from?\n",
    "df_ds = ds_tokens_known.to_pandas()\n",
    "df_ds[['ds_string','sys_instr_name']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = ds_tokens.select_columns(['ds_string', 'sys_instr_name', 'answer_choices', 'label_true', 'instructed_to_lie']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f71dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "print(df_metadata.iloc[i])\n",
    "# print(ds_tokens['formatted_chat'][i])\n",
    "print(tokenizer.decode(ds_tokens['input_ids'][i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc71615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_prompt(messages):\n",
    "    o = tokenizer.apply_chat_template(\n",
    "                messages, add_generation_prompt=True, return_tensors=\"pt\",\n",
    "            max_length=cfg.max_tokens,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_dict=True,\n",
    "            )\n",
    "    return {k:v.squeeze() for k,v in o.items()}\n",
    "\n",
    "ds_tokens = (\n",
    "    ds_prompts.map(\n",
    "        lambda x: apply_prompt(x[\"messages\"])\n",
    "    )\n",
    "    .map(lambda r: {\"choice_ids\": row_choice_ids(r, tokenizer)}, desc=\"choice_ids\")\n",
    "    .filter(lambda x: x[\"attention_mask\"].sum() < cfg.max_tokens)\n",
    ")\n",
    "ds_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "print(df_metadata.iloc[i])\n",
    "# print(ds_tokens['formatted_chat'][i])\n",
    "print(tokenizer.decode(ds_tokens['input_ids'][i], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
